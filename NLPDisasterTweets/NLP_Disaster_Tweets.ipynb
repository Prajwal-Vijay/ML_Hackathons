{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 17777,
          "databundleVersionId": 869809,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30748,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "23KZCyGmfSAE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "nlp_getting_started_path = kagglehub.competition_download('nlp-getting-started')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "BpULH0B1fSAF"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2025-06-10T15:30:04.553067Z",
          "iopub.execute_input": "2025-06-10T15:30:04.553362Z",
          "iopub.status.idle": "2025-06-10T15:30:06.970764Z",
          "shell.execute_reply.started": "2025-06-10T15:30:04.553332Z",
          "shell.execute_reply": "2025-06-10T15:30:06.969835Z"
        },
        "trusted": true,
        "id": "McSXfhONfSAF",
        "outputId": "e8a20445-a425-4b6d-bcca-bb317e024d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TFAutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(tf.__version__)\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-10T15:30:06.972864Z",
          "iopub.execute_input": "2025-06-10T15:30:06.973343Z",
          "iopub.status.idle": "2025-06-10T15:30:37.654479Z",
          "shell.execute_reply.started": "2025-06-10T15:30:06.973294Z",
          "shell.execute_reply": "2025-06-10T15:30:37.653532Z"
        },
        "trusted": true,
        "id": "naPRUoaefSAF",
        "outputId": "a0be8756-dc8f-4fd6-a2b9-a0b1fff48353"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-06-10 15:30:13.766184: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-06-10 15:30:13.766326: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-10 15:30:14.107312: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "2.15.0\n4.42.3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
        "X = df.drop(['target'],axis=1)\n",
        "y = df['target']\n",
        "test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
        "X_test = test_df.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-10T15:30:37.65559Z",
          "iopub.execute_input": "2025-06-10T15:30:37.656113Z",
          "iopub.status.idle": "2025-06-10T15:30:37.776519Z",
          "shell.execute_reply.started": "2025-06-10T15:30:37.656087Z",
          "shell.execute_reply": "2025-06-10T15:30:37.775819Z"
        },
        "trusted": true,
        "id": "LDFIXdKPfSAG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ids = list(X_test['id'])\n",
        "ids"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-10T15:30:37.777533Z",
          "iopub.execute_input": "2025-06-10T15:30:37.777801Z",
          "iopub.status.idle": "2025-06-10T15:30:37.794095Z",
          "shell.execute_reply.started": "2025-06-10T15:30:37.77778Z",
          "shell.execute_reply": "2025-06-10T15:30:37.793235Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "ZfOuKvt-fSAG",
        "outputId": "24a8375a-b98b-4024-8d3a-a4e909b8eb9c"
      },
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[0,\n 2,\n 3,\n 9,\n 11,\n 12,\n 21,\n 22,\n 27,\n 29,\n 30,\n 35,\n 42,\n 43,\n 45,\n 46,\n 47,\n 51,\n 58,\n 60,\n 69,\n 70,\n 72,\n 75,\n 84,\n 87,\n 88,\n 90,\n 94,\n 99,\n 101,\n 103,\n 106,\n 108,\n 111,\n 115,\n 116,\n 122,\n 123,\n 124,\n 125,\n 127,\n 140,\n 142,\n 147,\n 148,\n 150,\n 152,\n 154,\n 155,\n 166,\n 167,\n 169,\n 177,\n 179,\n 181,\n 186,\n 188,\n 189,\n 192,\n 200,\n 202,\n 206,\n 207,\n 214,\n 217,\n 223,\n 224,\n 227,\n 228,\n 230,\n 233,\n 234,\n 236,\n 239,\n 250,\n 255,\n 257,\n 259,\n 275,\n 278,\n 282,\n 284,\n 286,\n 288,\n 292,\n 295,\n 300,\n 304,\n 305,\n 306,\n 308,\n 311,\n 317,\n 319,\n 323,\n 324,\n 325,\n 326,\n 333,\n 339,\n 342,\n 343,\n 350,\n 351,\n 357,\n 359,\n 362,\n 366,\n 367,\n 369,\n 373,\n 374,\n 376,\n 377,\n 378,\n 379,\n 382,\n 385,\n 387,\n 388,\n 391,\n 392,\n 395,\n 399,\n 400,\n 403,\n 405,\n 408,\n 411,\n 414,\n 416,\n 417,\n 422,\n 425,\n 428,\n 430,\n 431,\n 433,\n 434,\n 439,\n 441,\n 449,\n 458,\n 460,\n 464,\n 473,\n 488,\n 491,\n 494,\n 497,\n 500,\n 505,\n 507,\n 508,\n 510,\n 511,\n 515,\n 525,\n 529,\n 532,\n 534,\n 537,\n 539,\n 541,\n 545,\n 547,\n 548,\n 549,\n 553,\n 554,\n 555,\n 557,\n 562,\n 566,\n 572,\n 573,\n 582,\n 586,\n 587,\n 590,\n 591,\n 593,\n 595,\n 596,\n 597,\n 601,\n 602,\n 605,\n 610,\n 616,\n 618,\n 620,\n 626,\n 627,\n 629,\n 632,\n 634,\n 639,\n 645,\n 647,\n 648,\n 650,\n 663,\n 666,\n 668,\n 670,\n 673,\n 676,\n 678,\n 692,\n 693,\n 694,\n 695,\n 696,\n 698,\n 701,\n 703,\n 707,\n 708,\n 711,\n 715,\n 718,\n 722,\n 723,\n 733,\n 741,\n 742,\n 743,\n 747,\n 749,\n 750,\n 756,\n 757,\n 760,\n 764,\n 765,\n 766,\n 768,\n 769,\n 771,\n 772,\n 776,\n 778,\n 780,\n 785,\n 789,\n 792,\n 793,\n 811,\n 813,\n 816,\n 821,\n 824,\n 825,\n 827,\n 830,\n 831,\n 839,\n 844,\n 847,\n 850,\n 854,\n 855,\n 858,\n 861,\n 862,\n 865,\n 869,\n 879,\n 880,\n 887,\n 889,\n 897,\n 900,\n 901,\n 904,\n 908,\n 909,\n 910,\n 913,\n 914,\n 917,\n 918,\n 920,\n 922,\n 924,\n 925,\n 927,\n 933,\n 937,\n 943,\n 949,\n 950,\n 954,\n 966,\n 967,\n 969,\n 970,\n 973,\n 975,\n 980,\n 988,\n 989,\n 995,\n 1000,\n 1003,\n 1007,\n 1011,\n 1012,\n 1013,\n 1014,\n 1016,\n 1019,\n 1025,\n 1027,\n 1028,\n 1030,\n 1033,\n 1034,\n 1039,\n 1046,\n 1047,\n 1053,\n 1055,\n 1056,\n 1059,\n 1060,\n 1063,\n 1064,\n 1068,\n 1076,\n 1086,\n 1087,\n 1089,\n 1092,\n 1095,\n 1096,\n 1097,\n 1100,\n 1101,\n 1107,\n 1108,\n 1111,\n 1115,\n 1116,\n 1121,\n 1125,\n 1127,\n 1131,\n 1133,\n 1135,\n 1137,\n 1140,\n 1144,\n 1147,\n 1148,\n 1150,\n 1158,\n 1159,\n 1161,\n 1163,\n 1165,\n 1169,\n 1171,\n 1172,\n 1176,\n 1180,\n 1184,\n 1186,\n 1187,\n 1192,\n 1193,\n 1194,\n 1197,\n 1200,\n 1205,\n 1210,\n 1216,\n 1220,\n 1231,\n 1233,\n 1246,\n 1247,\n 1248,\n 1255,\n 1256,\n 1257,\n 1258,\n 1260,\n 1261,\n 1265,\n 1266,\n 1268,\n 1274,\n 1281,\n 1285,\n 1286,\n 1291,\n 1292,\n 1295,\n 1299,\n 1306,\n 1310,\n 1311,\n 1313,\n 1314,\n 1322,\n 1323,\n 1325,\n 1329,\n 1330,\n 1333,\n 1336,\n 1339,\n 1342,\n 1344,\n 1355,\n 1357,\n 1358,\n 1359,\n 1364,\n 1366,\n 1367,\n 1370,\n 1373,\n 1377,\n 1386,\n 1387,\n 1392,\n 1397,\n 1398,\n 1400,\n 1403,\n 1404,\n 1410,\n 1413,\n 1416,\n 1417,\n 1423,\n 1424,\n 1426,\n 1427,\n 1428,\n 1430,\n 1434,\n 1435,\n 1437,\n 1438,\n 1442,\n 1446,\n 1451,\n 1457,\n 1461,\n 1462,\n 1465,\n 1468,\n 1469,\n 1471,\n 1476,\n 1478,\n 1481,\n 1489,\n 1490,\n 1492,\n 1496,\n 1512,\n 1516,\n 1517,\n 1528,\n 1529,\n 1536,\n 1539,\n 1541,\n 1542,\n 1548,\n 1550,\n 1551,\n 1552,\n 1557,\n 1563,\n 1564,\n 1565,\n 1566,\n 1571,\n 1578,\n 1581,\n 1583,\n 1584,\n 1586,\n 1589,\n 1592,\n 1598,\n 1606,\n 1612,\n 1616,\n 1620,\n 1624,\n 1629,\n 1630,\n 1635,\n 1640,\n 1641,\n 1642,\n 1651,\n 1655,\n 1656,\n 1659,\n 1664,\n 1667,\n 1668,\n 1674,\n 1678,\n 1680,\n 1681,\n 1682,\n 1685,\n 1695,\n 1696,\n 1697,\n 1704,\n 1708,\n 1711,\n 1713,\n 1714,\n 1717,\n 1729,\n 1730,\n 1732,\n 1734,\n 1736,\n 1738,\n 1742,\n 1743,\n 1746,\n 1748,\n 1749,\n 1751,\n 1758,\n 1764,\n 1765,\n 1777,\n 1778,\n 1781,\n 1782,\n 1783,\n 1785,\n 1788,\n 1793,\n 1794,\n 1795,\n 1797,\n 1800,\n 1801,\n 1805,\n 1806,\n 1819,\n 1820,\n 1825,\n 1828,\n 1829,\n 1830,\n 1839,\n 1843,\n 1844,\n 1846,\n 1849,\n 1850,\n 1854,\n 1855,\n 1858,\n 1859,\n 1862,\n 1867,\n 1868,\n 1871,\n 1872,\n 1874,\n 1876,\n 1879,\n 1884,\n 1891,\n 1894,\n 1896,\n 1902,\n 1903,\n 1904,\n 1906,\n 1907,\n 1912,\n 1913,\n 1923,\n 1926,\n 1928,\n 1930,\n 1931,\n 1934,\n 1936,\n 1944,\n 1946,\n 1947,\n 1958,\n 1964,\n 1970,\n 1974,\n 1977,\n 1978,\n 1982,\n 1984,\n 1988,\n 1993,\n 1997,\n 1998,\n 2002,\n 2004,\n 2005,\n 2008,\n 2011,\n 2013,\n 2018,\n 2021,\n 2025,\n 2029,\n 2030,\n 2032,\n 2037,\n 2041,\n 2044,\n 2048,\n 2052,\n 2053,\n 2054,\n 2062,\n 2065,\n 2066,\n 2072,\n 2079,\n 2080,\n 2085,\n 2088,\n 2090,\n 2092,\n 2093,\n 2101,\n 2104,\n 2105,\n 2106,\n 2107,\n 2120,\n 2124,\n 2127,\n 2130,\n 2132,\n 2135,\n 2137,\n 2140,\n 2143,\n 2147,\n 2151,\n 2152,\n 2155,\n 2156,\n 2162,\n 2165,\n 2166,\n 2167,\n 2168,\n 2170,\n 2178,\n 2180,\n 2182,\n 2184,\n 2185,\n 2187,\n 2196,\n 2197,\n 2199,\n 2200,\n 2201,\n 2202,\n 2206,\n 2208,\n 2218,\n 2223,\n 2224,\n 2226,\n 2228,\n 2232,\n 2234,\n 2243,\n 2247,\n 2249,\n 2252,\n 2253,\n 2259,\n 2261,\n 2264,\n 2268,\n 2269,\n 2270,\n 2276,\n 2283,\n 2287,\n 2290,\n 2291,\n 2293,\n 2295,\n 2302,\n 2305,\n 2310,\n 2313,\n 2316,\n 2320,\n 2322,\n 2323,\n 2326,\n 2328,\n 2331,\n 2335,\n 2338,\n 2343,\n 2344,\n 2345,\n 2353,\n 2355,\n 2357,\n 2360,\n 2365,\n 2369,\n 2371,\n 2378,\n 2380,\n 2381,\n 2383,\n 2384,\n 2392,\n 2393,\n 2401,\n 2403,\n 2404,\n 2405,\n 2407,\n 2411,\n 2424,\n 2426,\n 2431,\n 2433,\n 2434,\n 2436,\n 2439,\n 2444,\n 2447,\n 2448,\n 2449,\n 2450,\n 2461,\n 2469,\n 2472,\n 2473,\n 2474,\n 2477,\n 2481,\n 2484,\n 2495,\n 2503,\n 2509,\n 2511,\n 2518,\n 2522,\n 2525,\n 2526,\n 2529,\n 2533,\n 2549,\n 2551,\n 2558,\n 2562,\n 2563,\n 2567,\n 2574,\n 2577,\n 2578,\n 2580,\n 2581,\n 2583,\n 2584,\n 2586,\n 2589,\n 2595,\n 2596,\n 2600,\n 2601,\n 2607,\n 2610,\n 2613,\n 2615,\n 2618,\n 2620,\n 2623,\n 2626,\n 2630,\n 2634,\n 2636,\n 2638,\n 2639,\n 2646,\n 2650,\n 2652,\n 2653,\n 2654,\n 2662,\n 2665,\n 2669,\n 2674,\n 2678,\n 2681,\n 2685,\n 2686,\n 2690,\n 2697,\n 2699,\n 2704,\n 2705,\n 2710,\n 2712,\n 2713,\n 2716,\n 2717,\n 2718,\n 2721,\n 2722,\n 2735,\n 2737,\n 2738,\n 2742,\n 2745,\n 2746,\n 2747,\n 2750,\n 2751,\n 2754,\n 2762,\n 2764,\n 2772,\n 2775,\n 2776,\n 2779,\n 2781,\n 2789,\n 2790,\n 2791,\n 2798,\n 2800,\n 2804,\n 2805,\n 2806,\n 2809,\n 2810,\n 2812,\n 2814,\n 2816,\n 2818,\n 2823,\n 2824,\n 2834,\n 2837,\n 2840,\n 2845,\n 2847,\n 2848,\n 2850,\n 2859,\n 2862,\n 2868,\n 2874,\n 2876,\n 2892,\n 2894,\n 2897,\n 2901,\n 2903,\n 2904,\n 2906,\n 2914,\n 2918,\n 2919,\n 2923,\n 2926,\n 2928,\n 2930,\n 2938,\n 2940,\n 2949,\n 2951,\n 2958,\n 2961,\n 2962,\n 2964,\n 2966,\n 2967,\n 2968,\n 2972,\n 2977,\n 2978,\n 2979,\n 2981,\n 2985,\n 2986,\n 2989,\n 2994,\n 2996,\n 2997,\n 2999,\n 3002,\n 3007,\n 3008,\n 3017,\n 3020,\n 3024,\n 3028,\n 3031,\n 3032,\n 3033,\n 3035,\n 3040,\n 3041,\n 3046,\n 3050,\n 3063,\n 3065,\n 3067,\n 3069,\n 3076,\n 3078,\n 3081,\n 3087,\n 3088,\n 3094,\n 3096,\n 3098,\n 3103,\n 3110,\n 3113,\n 3121,\n 3127,\n 3128,\n 3129,\n 3135,\n 3143,\n 3146,\n 3148,\n 3149,\n 3151,\n 3156,\n 3160,\n 3164,\n 3169,\n 3178,\n 3187,\n 3194,\n 3202,\n 3203,\n 3204,\n 3206,\n 3207,\n 3208,\n 3209,\n 3213,\n 3214,\n 3215,\n 3220,\n 3223,\n 3224,\n 3226,\n 3228,\n 3230,\n 3232,\n 3233,\n 3234,\n 3238,\n 3246,\n 3247,\n 3250,\n 3251,\n 3254,\n 3257,\n 3258,\n 3261,\n 3267,\n 3268,\n 3269,\n 3271,\n 3272,\n 3273,\n 3279,\n 3290,\n 3291,\n 3293,\n 3294,\n 3298,\n ...]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Use regex to clean the data\n",
        "def remove_url(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',text)\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(table)\n",
        "def remove_html(text):\n",
        "    html=re.compile(r'<.*?>')\n",
        "    return html.sub(r'',text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def decontraction(text):\n",
        "    text = re.sub(r\"won\\'t\", \" will not\", text)\n",
        "    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n",
        "    text = re.sub(r\"can\\'t\", \" can not\", text)\n",
        "    text = re.sub(r\"don\\'t\", \" do not\", text)\n",
        "\n",
        "    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n",
        "    text = re.sub(r\"ma\\'am\", \" madam\", text)\n",
        "    text = re.sub(r\"let\\'s\", \" let us\", text)\n",
        "    text = re.sub(r\"ain\\'t\", \" am not\", text)\n",
        "    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n",
        "    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n",
        "    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n",
        "    text = re.sub(r\"y\\'all\", \" you all\", text)\n",
        "\n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"n\\'t've\", \" not have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'d've\", \" would have\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ll've\", \" will have\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    return text\n",
        "\n",
        "def seperate_alphanumeric(text):\n",
        "    words = text\n",
        "    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n",
        "    return \" \".join(words)\n",
        "\n",
        "def cont_rep_char(text):\n",
        "    tchr = text.group(0)\n",
        "\n",
        "    if len(tchr) > 1:\n",
        "        return tchr[0:2]\n",
        "\n",
        "def unique_char(rep, text):\n",
        "    substitute = re.sub(r'(\\w)\\1+', rep, text)\n",
        "    return substitute\n",
        "\n",
        "X['text'] = X['text'].apply(lambda x : remove_url(x))\n",
        "X['text'] = X['text'].apply(lambda x : remove_punct(x))\n",
        "X['text'] = X['text'].apply(lambda x : remove_emoji(x))\n",
        "X['text'] = X['text'].apply(lambda x : decontraction(x))\n",
        "X['text'] = X['text'].apply(lambda x : seperate_alphanumeric(x))\n",
        "X['text'] = X['text'].apply(lambda x : unique_char(cont_rep_char,x))\n",
        "\n",
        "X_test['text'] = X_test['text'].apply(lambda x : remove_url(x))\n",
        "X_test['text'] = X_test['text'].apply(lambda x : remove_punct(x))\n",
        "X_test['text'] = X_test['text'].apply(lambda x : remove_emoji(x))\n",
        "X_test['text'] = X_test['text'].apply(lambda x : decontraction(x))\n",
        "X_test['text'] = X_test['text'].apply(lambda x : seperate_alphanumeric(x))\n",
        "X_test['text'] = X_test['text'].apply(lambda x : unique_char(cont_rep_char,x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-10T15:34:45.383619Z",
          "iopub.execute_input": "2025-06-10T15:34:45.384065Z",
          "iopub.status.idle": "2025-06-10T15:34:45.823291Z",
          "shell.execute_reply.started": "2025-06-10T15:34:45.384028Z",
          "shell.execute_reply": "2025-06-10T15:34:45.822349Z"
        },
        "trusted": true,
        "id": "0hFfklEcfSAG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 256\n",
        "batch_size = 16\n",
        "num_samples = len(X)\n",
        "model_name = 'google-bert/bert-base-uncased'\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_tokens = tokenizer(\n",
        "    X['text'].tolist(),\n",
        "    max_length=seq_len,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    add_special_tokens=True,\n",
        "    return_tensors='np'\n",
        ")\n",
        "\n",
        "labels = np.zeros((num_samples, y.max() + 1))\n",
        "labels[np.arange(num_samples), y] = 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "        train_tokens['input_ids'],\n",
        "        train_tokens['attention_mask'],\n",
        "        labels\n",
        "    )\n",
        ")\n",
        "\n",
        "def map_func(input_ids, masks, labels):\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': masks\n",
        "    }, labels\n",
        "\n",
        "dataset = dataset.map(map_func)\n",
        "dataset = dataset.shuffle(10000).batch(batch_size=batch_size, drop_remainder=True)\n",
        "\n",
        "split = 0.6\n",
        "train_size = int((train_tokens['input_ids'].shape[0] // batch_size) * 0.7)\n",
        "val_size = int((train_tokens['input_ids'].shape[0] // batch_size) * 0.3)\n",
        "train_ds = dataset.take(train_size)\n",
        "val_ds = dataset.skip(train_size).take(val_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-10T15:34:51.689041Z",
          "iopub.execute_input": "2025-06-10T15:34:51.689809Z",
          "iopub.status.idle": "2025-06-10T15:34:54.210053Z",
          "shell.execute_reply.started": "2025-06-10T15:34:51.689782Z",
          "shell.execute_reply": "2025-06-10T15:34:54.209421Z"
        },
        "trusted": true,
        "id": "BZRZf3AmfSAH",
        "outputId": "0a2b0c2b-4b62-4fed-ddfe-c952e8951af4",
        "colab": {
          "referenced_widgets": [
            "56cdfdf892104880bc0eb2c3e1c51698",
            "916a866871a1492db246fa4ee590a43f",
            "30cc67271d6c4d12baf12e5816c8d596",
            "7b3c69617f304b098993a4d7a595c099"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56cdfdf892104880bc0eb2c3e1c51698"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "916a866871a1492db246fa4ee590a43f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30cc67271d6c4d12baf12e5816c8d596"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b3c69617f304b098993a4d7a595c099"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ids"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-10T15:35:02.834364Z",
          "iopub.execute_input": "2025-06-10T15:35:02.83466Z",
          "iopub.status.idle": "2025-06-10T15:35:02.848084Z",
          "shell.execute_reply.started": "2025-06-10T15:35:02.834638Z",
          "shell.execute_reply": "2025-06-10T15:35:02.847233Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "u7a2WqV9fSAH",
        "outputId": "5c9c435d-5e44-4da4-de9c-cc30f20b56fa"
      },
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[0,\n 2,\n 3,\n 9,\n 11,\n 12,\n 21,\n 22,\n 27,\n 29,\n 30,\n 35,\n 42,\n 43,\n 45,\n 46,\n 47,\n 51,\n 58,\n 60,\n 69,\n 70,\n 72,\n 75,\n 84,\n 87,\n 88,\n 90,\n 94,\n 99,\n 101,\n 103,\n 106,\n 108,\n 111,\n 115,\n 116,\n 122,\n 123,\n 124,\n 125,\n 127,\n 140,\n 142,\n 147,\n 148,\n 150,\n 152,\n 154,\n 155,\n 166,\n 167,\n 169,\n 177,\n 179,\n 181,\n 186,\n 188,\n 189,\n 192,\n 200,\n 202,\n 206,\n 207,\n 214,\n 217,\n 223,\n 224,\n 227,\n 228,\n 230,\n 233,\n 234,\n 236,\n 239,\n 250,\n 255,\n 257,\n 259,\n 275,\n 278,\n 282,\n 284,\n 286,\n 288,\n 292,\n 295,\n 300,\n 304,\n 305,\n 306,\n 308,\n 311,\n 317,\n 319,\n 323,\n 324,\n 325,\n 326,\n 333,\n 339,\n 342,\n 343,\n 350,\n 351,\n 357,\n 359,\n 362,\n 366,\n 367,\n 369,\n 373,\n 374,\n 376,\n 377,\n 378,\n 379,\n 382,\n 385,\n 387,\n 388,\n 391,\n 392,\n 395,\n 399,\n 400,\n 403,\n 405,\n 408,\n 411,\n 414,\n 416,\n 417,\n 422,\n 425,\n 428,\n 430,\n 431,\n 433,\n 434,\n 439,\n 441,\n 449,\n 458,\n 460,\n 464,\n 473,\n 488,\n 491,\n 494,\n 497,\n 500,\n 505,\n 507,\n 508,\n 510,\n 511,\n 515,\n 525,\n 529,\n 532,\n 534,\n 537,\n 539,\n 541,\n 545,\n 547,\n 548,\n 549,\n 553,\n 554,\n 555,\n 557,\n 562,\n 566,\n 572,\n 573,\n 582,\n 586,\n 587,\n 590,\n 591,\n 593,\n 595,\n 596,\n 597,\n 601,\n 602,\n 605,\n 610,\n 616,\n 618,\n 620,\n 626,\n 627,\n 629,\n 632,\n 634,\n 639,\n 645,\n 647,\n 648,\n 650,\n 663,\n 666,\n 668,\n 670,\n 673,\n 676,\n 678,\n 692,\n 693,\n 694,\n 695,\n 696,\n 698,\n 701,\n 703,\n 707,\n 708,\n 711,\n 715,\n 718,\n 722,\n 723,\n 733,\n 741,\n 742,\n 743,\n 747,\n 749,\n 750,\n 756,\n 757,\n 760,\n 764,\n 765,\n 766,\n 768,\n 769,\n 771,\n 772,\n 776,\n 778,\n 780,\n 785,\n 789,\n 792,\n 793,\n 811,\n 813,\n 816,\n 821,\n 824,\n 825,\n 827,\n 830,\n 831,\n 839,\n 844,\n 847,\n 850,\n 854,\n 855,\n 858,\n 861,\n 862,\n 865,\n 869,\n 879,\n 880,\n 887,\n 889,\n 897,\n 900,\n 901,\n 904,\n 908,\n 909,\n 910,\n 913,\n 914,\n 917,\n 918,\n 920,\n 922,\n 924,\n 925,\n 927,\n 933,\n 937,\n 943,\n 949,\n 950,\n 954,\n 966,\n 967,\n 969,\n 970,\n 973,\n 975,\n 980,\n 988,\n 989,\n 995,\n 1000,\n 1003,\n 1007,\n 1011,\n 1012,\n 1013,\n 1014,\n 1016,\n 1019,\n 1025,\n 1027,\n 1028,\n 1030,\n 1033,\n 1034,\n 1039,\n 1046,\n 1047,\n 1053,\n 1055,\n 1056,\n 1059,\n 1060,\n 1063,\n 1064,\n 1068,\n 1076,\n 1086,\n 1087,\n 1089,\n 1092,\n 1095,\n 1096,\n 1097,\n 1100,\n 1101,\n 1107,\n 1108,\n 1111,\n 1115,\n 1116,\n 1121,\n 1125,\n 1127,\n 1131,\n 1133,\n 1135,\n 1137,\n 1140,\n 1144,\n 1147,\n 1148,\n 1150,\n 1158,\n 1159,\n 1161,\n 1163,\n 1165,\n 1169,\n 1171,\n 1172,\n 1176,\n 1180,\n 1184,\n 1186,\n 1187,\n 1192,\n 1193,\n 1194,\n 1197,\n 1200,\n 1205,\n 1210,\n 1216,\n 1220,\n 1231,\n 1233,\n 1246,\n 1247,\n 1248,\n 1255,\n 1256,\n 1257,\n 1258,\n 1260,\n 1261,\n 1265,\n 1266,\n 1268,\n 1274,\n 1281,\n 1285,\n 1286,\n 1291,\n 1292,\n 1295,\n 1299,\n 1306,\n 1310,\n 1311,\n 1313,\n 1314,\n 1322,\n 1323,\n 1325,\n 1329,\n 1330,\n 1333,\n 1336,\n 1339,\n 1342,\n 1344,\n 1355,\n 1357,\n 1358,\n 1359,\n 1364,\n 1366,\n 1367,\n 1370,\n 1373,\n 1377,\n 1386,\n 1387,\n 1392,\n 1397,\n 1398,\n 1400,\n 1403,\n 1404,\n 1410,\n 1413,\n 1416,\n 1417,\n 1423,\n 1424,\n 1426,\n 1427,\n 1428,\n 1430,\n 1434,\n 1435,\n 1437,\n 1438,\n 1442,\n 1446,\n 1451,\n 1457,\n 1461,\n 1462,\n 1465,\n 1468,\n 1469,\n 1471,\n 1476,\n 1478,\n 1481,\n 1489,\n 1490,\n 1492,\n 1496,\n 1512,\n 1516,\n 1517,\n 1528,\n 1529,\n 1536,\n 1539,\n 1541,\n 1542,\n 1548,\n 1550,\n 1551,\n 1552,\n 1557,\n 1563,\n 1564,\n 1565,\n 1566,\n 1571,\n 1578,\n 1581,\n 1583,\n 1584,\n 1586,\n 1589,\n 1592,\n 1598,\n 1606,\n 1612,\n 1616,\n 1620,\n 1624,\n 1629,\n 1630,\n 1635,\n 1640,\n 1641,\n 1642,\n 1651,\n 1655,\n 1656,\n 1659,\n 1664,\n 1667,\n 1668,\n 1674,\n 1678,\n 1680,\n 1681,\n 1682,\n 1685,\n 1695,\n 1696,\n 1697,\n 1704,\n 1708,\n 1711,\n 1713,\n 1714,\n 1717,\n 1729,\n 1730,\n 1732,\n 1734,\n 1736,\n 1738,\n 1742,\n 1743,\n 1746,\n 1748,\n 1749,\n 1751,\n 1758,\n 1764,\n 1765,\n 1777,\n 1778,\n 1781,\n 1782,\n 1783,\n 1785,\n 1788,\n 1793,\n 1794,\n 1795,\n 1797,\n 1800,\n 1801,\n 1805,\n 1806,\n 1819,\n 1820,\n 1825,\n 1828,\n 1829,\n 1830,\n 1839,\n 1843,\n 1844,\n 1846,\n 1849,\n 1850,\n 1854,\n 1855,\n 1858,\n 1859,\n 1862,\n 1867,\n 1868,\n 1871,\n 1872,\n 1874,\n 1876,\n 1879,\n 1884,\n 1891,\n 1894,\n 1896,\n 1902,\n 1903,\n 1904,\n 1906,\n 1907,\n 1912,\n 1913,\n 1923,\n 1926,\n 1928,\n 1930,\n 1931,\n 1934,\n 1936,\n 1944,\n 1946,\n 1947,\n 1958,\n 1964,\n 1970,\n 1974,\n 1977,\n 1978,\n 1982,\n 1984,\n 1988,\n 1993,\n 1997,\n 1998,\n 2002,\n 2004,\n 2005,\n 2008,\n 2011,\n 2013,\n 2018,\n 2021,\n 2025,\n 2029,\n 2030,\n 2032,\n 2037,\n 2041,\n 2044,\n 2048,\n 2052,\n 2053,\n 2054,\n 2062,\n 2065,\n 2066,\n 2072,\n 2079,\n 2080,\n 2085,\n 2088,\n 2090,\n 2092,\n 2093,\n 2101,\n 2104,\n 2105,\n 2106,\n 2107,\n 2120,\n 2124,\n 2127,\n 2130,\n 2132,\n 2135,\n 2137,\n 2140,\n 2143,\n 2147,\n 2151,\n 2152,\n 2155,\n 2156,\n 2162,\n 2165,\n 2166,\n 2167,\n 2168,\n 2170,\n 2178,\n 2180,\n 2182,\n 2184,\n 2185,\n 2187,\n 2196,\n 2197,\n 2199,\n 2200,\n 2201,\n 2202,\n 2206,\n 2208,\n 2218,\n 2223,\n 2224,\n 2226,\n 2228,\n 2232,\n 2234,\n 2243,\n 2247,\n 2249,\n 2252,\n 2253,\n 2259,\n 2261,\n 2264,\n 2268,\n 2269,\n 2270,\n 2276,\n 2283,\n 2287,\n 2290,\n 2291,\n 2293,\n 2295,\n 2302,\n 2305,\n 2310,\n 2313,\n 2316,\n 2320,\n 2322,\n 2323,\n 2326,\n 2328,\n 2331,\n 2335,\n 2338,\n 2343,\n 2344,\n 2345,\n 2353,\n 2355,\n 2357,\n 2360,\n 2365,\n 2369,\n 2371,\n 2378,\n 2380,\n 2381,\n 2383,\n 2384,\n 2392,\n 2393,\n 2401,\n 2403,\n 2404,\n 2405,\n 2407,\n 2411,\n 2424,\n 2426,\n 2431,\n 2433,\n 2434,\n 2436,\n 2439,\n 2444,\n 2447,\n 2448,\n 2449,\n 2450,\n 2461,\n 2469,\n 2472,\n 2473,\n 2474,\n 2477,\n 2481,\n 2484,\n 2495,\n 2503,\n 2509,\n 2511,\n 2518,\n 2522,\n 2525,\n 2526,\n 2529,\n 2533,\n 2549,\n 2551,\n 2558,\n 2562,\n 2563,\n 2567,\n 2574,\n 2577,\n 2578,\n 2580,\n 2581,\n 2583,\n 2584,\n 2586,\n 2589,\n 2595,\n 2596,\n 2600,\n 2601,\n 2607,\n 2610,\n 2613,\n 2615,\n 2618,\n 2620,\n 2623,\n 2626,\n 2630,\n 2634,\n 2636,\n 2638,\n 2639,\n 2646,\n 2650,\n 2652,\n 2653,\n 2654,\n 2662,\n 2665,\n 2669,\n 2674,\n 2678,\n 2681,\n 2685,\n 2686,\n 2690,\n 2697,\n 2699,\n 2704,\n 2705,\n 2710,\n 2712,\n 2713,\n 2716,\n 2717,\n 2718,\n 2721,\n 2722,\n 2735,\n 2737,\n 2738,\n 2742,\n 2745,\n 2746,\n 2747,\n 2750,\n 2751,\n 2754,\n 2762,\n 2764,\n 2772,\n 2775,\n 2776,\n 2779,\n 2781,\n 2789,\n 2790,\n 2791,\n 2798,\n 2800,\n 2804,\n 2805,\n 2806,\n 2809,\n 2810,\n 2812,\n 2814,\n 2816,\n 2818,\n 2823,\n 2824,\n 2834,\n 2837,\n 2840,\n 2845,\n 2847,\n 2848,\n 2850,\n 2859,\n 2862,\n 2868,\n 2874,\n 2876,\n 2892,\n 2894,\n 2897,\n 2901,\n 2903,\n 2904,\n 2906,\n 2914,\n 2918,\n 2919,\n 2923,\n 2926,\n 2928,\n 2930,\n 2938,\n 2940,\n 2949,\n 2951,\n 2958,\n 2961,\n 2962,\n 2964,\n 2966,\n 2967,\n 2968,\n 2972,\n 2977,\n 2978,\n 2979,\n 2981,\n 2985,\n 2986,\n 2989,\n 2994,\n 2996,\n 2997,\n 2999,\n 3002,\n 3007,\n 3008,\n 3017,\n 3020,\n 3024,\n 3028,\n 3031,\n 3032,\n 3033,\n 3035,\n 3040,\n 3041,\n 3046,\n 3050,\n 3063,\n 3065,\n 3067,\n 3069,\n 3076,\n 3078,\n 3081,\n 3087,\n 3088,\n 3094,\n 3096,\n 3098,\n 3103,\n 3110,\n 3113,\n 3121,\n 3127,\n 3128,\n 3129,\n 3135,\n 3143,\n 3146,\n 3148,\n 3149,\n 3151,\n 3156,\n 3160,\n 3164,\n 3169,\n 3178,\n 3187,\n 3194,\n 3202,\n 3203,\n 3204,\n 3206,\n 3207,\n 3208,\n 3209,\n 3213,\n 3214,\n 3215,\n 3220,\n 3223,\n 3224,\n 3226,\n 3228,\n 3230,\n 3232,\n 3233,\n 3234,\n 3238,\n 3246,\n 3247,\n 3250,\n 3251,\n 3254,\n 3257,\n 3258,\n 3261,\n 3267,\n 3268,\n 3269,\n 3271,\n 3272,\n 3273,\n 3279,\n 3290,\n 3291,\n 3293,\n 3294,\n 3298,\n ...]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_tokens = tokenizer(\n",
        "    X_test['text'].tolist(),\n",
        "    max_length=seq_len,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    add_special_tokens=True,\n",
        "    return_tensors='np'\n",
        ")\n",
        "\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "        test_tokens['input_ids'],\n",
        "        test_tokens['attention_mask']\n",
        "    )\n",
        ")\n",
        "\n",
        "def map_func(input_ids, masks):\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': masks\n",
        "    }\n",
        "test_dataset = test_dataset.map(map_func)\n",
        "test_dataset = test_dataset.batch(batch_size=batch_size)\n",
        "print(list(test_dataset.as_numpy_iterator())[0])\n",
        "\n",
        "test_ds = test_dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-10T15:35:11.328115Z",
          "iopub.execute_input": "2025-06-10T15:35:11.328818Z",
          "iopub.status.idle": "2025-06-10T15:35:11.845931Z",
          "shell.execute_reply.started": "2025-06-10T15:35:11.328791Z",
          "shell.execute_reply": "2025-06-10T15:35:11.844973Z"
        },
        "trusted": true,
        "id": "PeKQULM_fSAH",
        "outputId": "0f93aac1-69c3-4423-89c3-76cc96058904"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'input_ids': array([[  101,  2074,  3047, ...,     0,     0,     0],\n       [  101,  2657,  2055, ...,     0,     0,     0],\n       [  101,  2045,  2003, ...,     0,     0,     0],\n       ...,\n       [  101,  2054,  2065, ...,     0,     0,     0],\n       [  101, 12476,   102, ...,     0,     0,     0],\n       [  101,  6484, 17264, ...,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0]])}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFAutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
        "\n",
        "# Two inputs||\n",
        "input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
        "mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
        "\n",
        "# Transformer\n",
        "# embeddings = model.bert(input_ids, attention_mask=mask)[1]\n",
        "embeddings = model(input_ids, attention_mask=mask).last_hidden_state\n",
        "# We are considering only the CLS token (the first token) for classification, as that is a good representation of the aggregate, and is commonly used for classification\n",
        "# This is infact known as pooler output, this has a shape of [batch_size, 1, hidden_size]\n",
        "embeddings = embeddings[:, 0, :]\n",
        "# Classifier head\n",
        "x = tf.keras.layers.Dense(512, activation='relu')(embeddings)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\n",
        "\n",
        "bert_model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "\n",
        "# freeze bert layers\n",
        "# bert_model.layers[2].trainable = False\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "acc = tf.keras.metrics.BinaryAccuracy()\n",
        "\n",
        "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
        "\n",
        "history = bert_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=1,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-10T15:35:15.429611Z",
          "iopub.execute_input": "2025-06-10T15:35:15.430466Z"
        },
        "trusted": true,
        "id": "60nggPAgfSAH",
        "outputId": "5bee0351-fa88-4ca5-abae-e9a2c627a780",
        "colab": {
          "referenced_widgets": [
            "8afd325a13b04ddb8656e0f76822854b"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8afd325a13b04ddb8656e0f76822854b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1749569778.394796     118 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": " 21/332 [>.............................] - ETA: 4:02 - loss: 0.7194 - binary_accuracy: 0.5685",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_pool = model(input_ids, attention_mask=mask).pooler_output\n",
        "print(embeddings_pool)\n",
        "x = tf.keras.layers.Dense(512, activation='relu')(embeddings_pool)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\n",
        "\n",
        "bert_model_pooler = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "acc = tf.keras.metrics.BinaryAccuracy()\n",
        "\n",
        "bert_model_pooler.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
        "\n",
        "history = bert_model_pooler.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=1,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "883wOwbPfSAI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model(input_ids, attention_mask=mask).last_hidden_state\n",
        "embeddings_averaged = embeddings[:,0,:]\n",
        "for i in range(1,256):\n",
        "    embeddings_averaged += embeddings[:,i,:]\n",
        "embeddings_averaged = embeddings_averaged/256\n",
        "print(embeddings_averaged.shape)\n",
        "x = tf.keras.layers.Dense(512, activation='relu')(embeddings_averaged)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\n",
        "\n",
        "bert_model_averaged = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "acc = tf.keras.metrics.BinaryAccuracy()\n",
        "optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=0.1)\n",
        "bert_model_averaged.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
        "history = bert_model_averaged.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=1,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "iOQGojBNfSAI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_CLS = bert_model.predict(test_ds)\n",
        "predictions_avg = bert_model_averaged.predict(test_ds)\n",
        "predictions_pooler = bert_model_pooler.predict(test_ds)"
      ],
      "metadata": {
        "trusted": true,
        "id": "iX51h3ZZfSAI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_list = []\n",
        "for prediction in predictions_CLS:\n",
        "    if prediction[0] > prediction[1]:\n",
        "        predicted_list.append(0)\n",
        "    else:\n",
        "        predicted_list.append(1)\n",
        "rdf = pd.DataFrame({'id':ids, 'target': predicted_list})\n",
        "rdf.to_csv('submission_4_CLS.csv',index=False)\n",
        "predicted_list = []\n",
        "for prediction in predictions_avg:\n",
        "    if prediction[0] > prediction[1]:\n",
        "        predicted_list.append(0)\n",
        "    else:\n",
        "        predicted_list.append(1)\n",
        "rdf = pd.DataFrame({'id':ids, 'target': predicted_list})\n",
        "rdf.to_csv('submission_4_avg.csv',index=False)\n",
        "predicted_list = []\n",
        "for prediction in predictions_pooler:\n",
        "    if prediction[0] > prediction[1]:\n",
        "        predicted_list.append(0)\n",
        "    else:\n",
        "        predicted_list.append(1)\n",
        "rdf = pd.DataFrame({'id':ids,  'target': predicted_list})\n",
        "rdf.to_csv('submission_4_pool.csv',index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "buxX5mq-fSAI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhPtb5rrfSAI"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}